{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae9de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest():\n",
    "    loader = PyPDFLoader(r\"input_docs/Chip Huyen - Designing Machine Learning Systems_ An Iterative Process for Production-Ready Applications (2022, O'Reilly Media) - libgen.li.pdf\")\n",
    "    pages = loader.load_and_split()\n",
    "   \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split {len(pages)} documents into {len(chunks)} chunks.\")\n",
    "   \n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    \n",
    "    Chroma.from_documents(documents=chunks,  embedding=embedding, persist_directory=\"./sql_chroma_db3\")\n",
    "\n",
    "#Chroma, fastembed, text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43a62b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 385 documents into 1075 chunks.\n"
     ]
    }
   ],
   "source": [
    "ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_read = \"your_token\"\n",
    "access_token_write = \"your_token\"\n",
    "login(token = access_token_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06154a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_chain():\n",
    "    model = ChatOllama(model=\"llama3\")\n",
    "    \n",
    "    \n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        <s> [Instructions] You are a friendly assistant. Answer the question based only on the following context and try to search the context as much as you can. \n",
    "        If you don't know the answer, then reply, No Context available for this question {input}. [/Instructions] </s> \n",
    "        [Instructions] Question: {input} \n",
    "        Context: {context} \n",
    "        Answer: [/Instructions]\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    vector_store = Chroma(persist_directory=\"./sql_chroma_db3\", embedding_function=embedding)\n",
    "\n",
    "    \n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"score_threshold\": 0.3,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    document_chain = create_stuff_documents_chain(model, prompt)\n",
    "    chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a297bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query: str):\n",
    "    \n",
    "    chain = rag_chain()\n",
    "    \n",
    "    result = chain.invoke({\"input\": query})\n",
    "    \n",
    "    print(result[\"answer\"])\n",
    "    for doc in result[\"context\"]:\n",
    "        print(\"Source: \", doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26fcb413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I'll prepare some questions for Chapter 5 Feature Engineering. Here are a few:\n",
      "\n",
      "1. What are some key aspects of feature engineering that will be covered in this chapter?\n",
      "2. How do multiple models sharing a feature affect its computation, and what are some considerations to take into account?\n",
      "3. What is data leakage, and how can it be detected and avoided in the context of feature engineering?\n",
      "4. How do feature stores fit into the broader landscape of machine learning applications, and when will they be discussed further?\n",
      "5. What are the two aspects that will be focused on in this chapter regarding extracting features from raw data for input into an ML model?\n",
      "\n",
      "Let me know if you'd like me to help with anything else!\n",
      "Source:  input_docs/Chip Huyen - Designing Machine Learning Systems_ An Iterative Process for Production-Ready Applications (2022, O'Reilly Media) - libgen.li.pdf\n",
      "Source:  input_docs/Chip Huyen - Designing Machine Learning Systems_ An Iterative Process for Production-Ready Applications (2022, O'Reilly Media) - libgen.li.pdf\n",
      "Source:  input_docs/Chip Huyen - Designing Machine Learning Systems_ An Iterative Process for Production-Ready Applications (2022, O'Reilly Media) - libgen.li.pdf\n"
     ]
    }
   ],
   "source": [
    "ask(\"Prepare some questions for Chapter 5 Feature Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "497d4b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help! Since the context provides information about the book's chapters and contents, I can give you a summary of the key concepts for each chapter based on what's available.\n",
      "\n",
      "Unfortunately, there is no Epilogue chapter mentioned in the provided context. Therefore, I won't include it in my answer.\n",
      "\n",
      "Here are some key concepts for each chapter:\n",
      "\n",
      "* Chapters related to Statistical concepts:\n",
      "\t+ Variance\n",
      "\t+ Probability\n",
      "\t+ Normal/Long-tail distribution\n",
      "\n",
      "Note that these chapters likely cover the basics of statistical concepts as they relate to machine learning.\n",
      "\n",
      "* Machine Learning (ML) tasks and concepts:\n",
      "\t+ Language modeling\n",
      "\t+ Anomaly detection\n",
      "\t+ Object classification\n",
      "\t+ Machine translation\n",
      "\n",
      "These chapters will likely delve into the various ML algorithms and techniques used for these tasks, along with their applications and limitations.\n",
      "\n",
      "Keep in mind that this is just a rough summary based on the provided context. For more detailed information about each chapter's key concepts, you may need to read the book itself or consult additional resources.\n",
      "Source:  input_docs/Chip Huyen - Designing Machine Learning Systems_ An Iterative Process for Production-Ready Applications (2022, O'Reilly Media) - libgen.li.pdf\n",
      "Source:  input_docs/Chip Huyen - Designing Machine Learning Systems_ An Iterative Process for Production-Ready Applications (2022, O'Reilly Media) - libgen.li.pdf\n",
      "Source:  input_docs/Chip Huyen - Designing Machine Learning Systems_ An Iterative Process for Production-Ready Applications (2022, O'Reilly Media) - libgen.li.pdf\n"
     ]
    }
   ],
   "source": [
    "ask(\"Give me Key concepts for each Chapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b03b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
